{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31e5947",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Example: save all conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.3.27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: where is the captial of Portugal?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Lisbon is the capital city of Portugal. It's located on the Tagus River, which makes it easy for ships and boats to come in and out. Would you like to know more about Lisbon?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no need of output parser since the output of the conversationchain is already string\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOllama(model=\"phi\")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "result = conversation.invoke(\"where is the captial of Portugal?\")       # print result to see it, it's a dictionary.\n",
    "print(result['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620839cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need of output parser since the output of the conversationchain is already string\n",
    "# if we want to use the prompt, no need to use ConversationChain, use RunnableWithMessageHistory\n",
    "# RunnableWithMessageHistory: use session id to save the conversation + handle multiple sessions (users) + custom prompt\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOllama(model=\"phi\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're an Assistant. Answer questions very breifly\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),                        # history of previous messages injected to the new message\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)    # save messages as list of messages (not string)\n",
    "\n",
    "\n",
    "chain = RunnableWithMessageHistory(\n",
    "    prompt | llm,\n",
    "    lambda session_id: memory,          # how to get memory\n",
    "    input_messages_key=\"input\",         # where new user message is\n",
    "    history_messages_key=\"history\"      # where chat history goes\n",
    ")\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "    {\"input\": \"Where is the capital of Portugal?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"test\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55033671",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "note 1: conversation | StrOutputParser() is wrong, since the output of the conversation chain is a string and the input of the parser should be a message\n",
    "note 2: default prompt > use ConversationChain, custom prompt > use RunnableWithMessageHistory\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884df5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
