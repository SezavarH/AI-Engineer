{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f511f02",
   "metadata": {},
   "source": [
    "## Ingestion Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c333cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company has 4 documents\n",
      "contracts has 32 documents\n",
      "employees has 32 documents\n",
      "products has 8 documents\n",
      "\n",
      "total number of documents 76\n",
      "\n",
      "num. chunks 970\n",
      "\n",
      "number of embedding vectors 970\n",
      "dimensions : 768\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================================================\n",
    "#                                               Loading Documents\n",
    "# ===============================================================================================================\n",
    "\n",
    "# documents of all subdirectory\n",
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "root = \"data\\knowledge-base\"\n",
    "sub_roots = glob.glob(root + \"\\*\")\n",
    "\n",
    "documents = []\n",
    "for sub_root in sub_roots:\n",
    "    sub_name = sub_root.split(\"\\\\\")[-1]\n",
    "    loader = DirectoryLoader(sub_root, glob = \"*.md\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "    docs   = loader.load()\n",
    "    # print(len(docs))\n",
    "\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"doc_type\"] = sub_name\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"{sub_name} has {len(docs)} documents\")\n",
    "\n",
    "\n",
    "print(f\"\\ntotal number of documents {len(documents)}\")\n",
    "\n",
    "\n",
    "# ===============================================================================================================\n",
    "#                                               Documents into Chunk\n",
    "# ===============================================================================================================\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nnum. chunks {len(chunks)}\")\n",
    "\n",
    "# ===============================================================================================================\n",
    "#                                               Vectorize and Store\n",
    "# ===============================================================================================================\n",
    "\n",
    "# an embedding model and a vectorDB\n",
    "\n",
    "## model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "## VectorDB\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "DB_NAME = \"./vector_db\"\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "        Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents( documents=chunks, embedding=embeddings, persist_directory=DB_NAME )\n",
    "\n",
    "# _collection is the internal Chroma object\n",
    "collection = vectorstore._collection\n",
    "\n",
    "# Count how many vectors exist  (should be as same as the num.chunks)\n",
    "num_embedding = collection.count()\n",
    "print(f\"\\nnumber of embedding vectors {num_embedding}\")\n",
    "\n",
    "assert len(chunks) == num_embedding\n",
    "\n",
    "# get one sample from DB\n",
    "sample = collection.get(limit = 1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "\n",
    "dimensions = len(sample)\n",
    "print(f\"dimensions : {dimensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1153f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e22247af",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f228d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "DB_NAME = \"./vector_db\"  \n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma(persist_directory = DB_NAME, embedding_function=embeddings)     # path, Embedding Model\n",
    "\n",
    "\n",
    "# when a query comes >> search and retrieve\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# define LLM model\n",
    "llm = ChatOllama(model=\"phi\",temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978295da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "If relevant, use the given context to answer any question.\n",
    "If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5c84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, convert_to_messages\n",
    "\n",
    "# in a chatbot, user may asks a question from db even in a middle of the converstaion\n",
    "\n",
    "# 1. combine query with the past converstation\n",
    "# 2. retrieve from db\n",
    "# 3. combine content of the retrieved documents\n",
    "# 4. add the new info the system prompt\n",
    "# 5. history looks like:\n",
    "#       history = [\n",
    "#           {\"role\": \"user\", \"content\": \"What is Insurellm?\"},\n",
    "#           {\"role\": \"assistant\", \"content\": \"Insurellm is...\"},\n",
    "#           ]\n",
    "# After conversion: convert_to_messages(history)\n",
    "# it looks like:\n",
    "#   [\n",
    "#       HumanMessage(\"What is Insurellm?\"),\n",
    "#       AIMessage(\"Insurellm is...\")\n",
    "#   ]\n",
    "# because history is multiple messages :: use extend, but HumanMessage is only one :: use append\n",
    "#\n",
    "# 6. ask llm to generate response\n",
    "\n",
    "\n",
    "def answer(query, history):\n",
    "\n",
    "    # 1\n",
    "    prior    =  \"\\n\".join(m[\"content\"] for m in history if m[\"role\"] == \"user\")\n",
    "    combined = f\"{prior}\\n{query}\" if prior else query\n",
    "\n",
    "    # 2\n",
    "    docs =    retriever.invoke(combined, k=3)\n",
    "\n",
    "    # 3\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # 4\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "\n",
    "    # 5\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.extend(convert_to_messages(history))\n",
    "    messages.append(HumanMessage(content=query))\n",
    "\n",
    "    # 6\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return response.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4492d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "  Insurellm is a company that provides insurance solutions for various industries. We offer comprehensive coverage options to protect businesses and individuals from financial losses due to unexpected events such as accidents, theft, or natural disasters. Our team of experts works closely with our clients to understand their unique needs and provide customized insurance plans that meet their requirements.\n",
      "\n",
      "\n",
      "SOURCES:\n",
      "1. data\\knowledge-base\\products\\Homellm.md\n",
      "2. data\\knowledge-base\\products\\Homellm.md\n",
      "3. data\\knowledge-base\\contracts\\Contract with Velocity Auto Solutions for Carllm.md\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    answer_text, docs = answer(\n",
    "        query=\"What is Insurellm?\",\n",
    "        history=[]\n",
    "    )\n",
    "\n",
    "    print(\"ANSWER:\\n\", answer_text)\n",
    "    print(\"\\nSOURCES:\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"{i}. {doc.metadata.get('source')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9867ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1:  Insurellm is a company that provides insurance solutions for various industries. They offer a range of products and services to help businesses protect their assets, employees, and customers from unexpected events. Their team of experts works closely with clients to understand their unique needs and develop customized insurance plans that provide peace of mind and financial security.\n",
      "\n",
      "A2:  Insurellm is for anyone who wants to protect themselves or their business from potential risks and liabilities. Whether you are an individual looking for personal insurance, a small business owner seeking coverage for your assets and employees, or a large corporation requiring comprehensive risk management solutions, Insurellm has the expertise and experience to help you find the right insurance products that meet your specific needs.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "q1 = \"What is Insurellm?\"\n",
    "a1, docs1 = answer(q1, history)\n",
    "print(\"A1:\", a1)\n",
    "\n",
    "history.append({\"role\": \"user\", \"content\": q1})\n",
    "history.append({\"role\": \"assistant\", \"content\": a1})\n",
    "\n",
    "q2 = \"Who is it for?\"\n",
    "a2, docs2 = answer(q2, history)\n",
    "print(\"A2:\", a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676da79",
   "metadata": {},
   "source": [
    "## Step by Step explaination of the answer part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What history looks like\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Insurellm?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Insurellm is...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is it for?\"}\n",
    "]\n",
    "\n",
    "# first step to answer  >> keep only user contents from previous conversations, ignore assistant\n",
    "\n",
    "m[\"content\"] for m in history if m[\"role\"] == \"user\"\n",
    "\n",
    "# new step: join them each in a new line\n",
    "prior = \"\\n\".join(...)\n",
    "\n",
    "# content is now:\n",
    "#   What is Insurellm?\n",
    "#   Who is it for?\n",
    "\n",
    "# now, a query comes :: join it \n",
    "combined = f\"{prior}\\n{query}\" if prior else query\n",
    "\n",
    "\n",
    "# new content is:\n",
    "#       What is Insurellm?\n",
    "#       Who is it for?\n",
    "#       Is it secure?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What retriever actually is :: a wrapper around Chroma\n",
    "\n",
    "docs = retriever.invoke(combined)\n",
    "\n",
    "# always “asks” the DB, because that’s the only place where your knowledge chunks live.\n",
    "# You always get something (never empty)\n",
    "# But the chunks may be irrelevant or nonsensical in context\n",
    "\n",
    "\n",
    "# How to handle irrelevant chunks?\n",
    "\n",
    "# Since retrieval always returns something, it’s the LLM’s job to ignore irrelevant context.\n",
    "\n",
    "# Your system prompt helps with this:\n",
    "\n",
    "\"If relevant, use the given context to answer any question. If you don't know the answer, say so.\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "If relevant, use the given context to answer any question.\n",
    "If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d70cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
