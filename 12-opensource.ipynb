{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open-Source vs Closed-Source LLMs\n",
        "\n",
        "\n",
        "\n",
        "**Learning goals**\n",
        "\n",
        "    Understand open-source vs closed-source LLMs\n",
        "\n",
        "    Load and run:\n",
        "\n",
        "    Open-source models via Hugging Face\n",
        "\n",
        "    Open-source models via Ollama (OpenAI-compatible API)\n",
        "\n",
        "    Closed-source models via OpenRouter\n",
        "\n",
        "    Inspect model properties\n",
        "\n",
        "    Send simple prompts and compare outputs"
      ],
      "metadata": {
        "id": "AXOYelqKzvjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate torch openai"
      ],
      "metadata": {
        "id": "RUsir95B0P7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Open Source + HuggingFace\n",
        "\n",
        "\n",
        "**Two ways:**\n",
        "\n",
        "    pipelines: https://huggingface.co/docs/transformers/en/main_classes/pipelines\n",
        "\n",
        "    Auto Classes: https://huggingface.co/docs/transformers/model_doc/auto"
      ],
      "metadata": {
        "id": "YMUmvglN0_ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pipeline"
      ],
      "metadata": {
        "id": "iSHVYsR62gFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y74oiEfazph7",
        "outputId": "ae57c543-b52b-498b-c8e2-9f48144e007e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# pipeline\n",
        "#             Great way to use models for inference\n",
        "#             Tasks: Audio      (classification, recognition),\n",
        "#                    Vision     (classification, obj detection, segmentation, depth)\n",
        "#                    NLP        (classification, Q&A, Summerization, Translation, Lang modeling(word prediciton))\n",
        "#                    MultiModal (Doc Q&A)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Audio classification\n",
        "pipe   = pipeline(\"audio-classification\")\n",
        "result = pipe(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ],
      "metadata": {
        "id": "UmzeK7Bk2exq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjjdV24O26KB",
        "outputId": "ae3a6b77-ee87-4eb6-8c32-464dee363489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.4868629574775696, 'label': 'up'},\n",
              " {'score': 0.19627460837364197, 'label': '_unknown_'},\n",
              " {'score': 0.19116656482219696, 'label': 'left'},\n",
              " {'score': 0.04208880290389061, 'label': '_silence_'},\n",
              " {'score': 0.0303605105727911, 'label': 'on'},\n",
              " {'score': 0.01467297226190567, 'label': 'go'},\n",
              " {'score': 0.013018293306231499, 'label': 'right'},\n",
              " {'score': 0.011818612925708294, 'label': 'off'},\n",
              " {'score': 0.005862788762897253, 'label': 'stop'},\n",
              " {'score': 0.005178585182875395, 'label': 'no'},\n",
              " {'score': 0.0020859132055193186, 'label': 'down'},\n",
              " {'score': 0.0006093769334256649, 'label': 'yes'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vision\n",
        "pipe   = pipeline(\"image-classification\")\n",
        "result = pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211,
          "referenced_widgets": [
            "ce5eb479a4cc4d8f98aac8eb2e0e2c15"
          ]
        },
        "id": "kjajo1rR3ABA",
        "outputId": "f5d5eaa8-2d1b-4d91-fef8-1663afbcdcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce5eb479a4cc4d8f98aac8eb2e0e2c15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'lynx, catamount', 'score': 0.43349990248680115},\n",
              " {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
              "  'score': 0.03479622304439545},\n",
              " {'label': 'snow leopard, ounce, Panthera uncia',\n",
              "  'score': 0.032401926815509796},\n",
              " {'label': 'Egyptian cat', 'score': 0.023944783955812454},\n",
              " {'label': 'tiger cat', 'score': 0.02288925088942051}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP and Q&A\n",
        "pipe = pipeline(\"question-answering\")\n",
        "result = pipe(question=\"Where do I work?\", context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkq7zIkx3RHL",
        "outputId": "82261b11-a701-4f66-d6a9-eb76f6455745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.6949766278266907, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZC6suLA3miK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Classes\n"
      ],
      "metadata": {
        "id": "R2vuwhFK5qoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto Classes\n",
        "\n",
        "#     pass name or path >> load model and its components\n",
        "#     AutoConfig, AutoTokenizer, AutoModel ...\n",
        "#     AutoModel is Generic class\n",
        "#     for NLP:    AutoModelForCausalLM, AutoModelForMaskLM, AutoModelForSequenceClassification, ...\n",
        "#     for Vision: AutoModelForImageClassification, AutoModelForDepthEstimation\n",
        "#     available for Audio, MutliModal, Timeseries and etc"
      ],
      "metadata": {
        "id": "kVqZxNA55ukZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"google/gemma-2b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"model name: {model_name}\")\n",
        "print(f\"num. parameters: {model.num_parameters()}\")\n",
        "print(\"Max context length:\", model.config.max_position_embeddings)\n",
        "\n",
        "# run a prompt\n",
        "prompt = \"Hi there, where is the capital of France?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.1,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(decoded)\n"
      ],
      "metadata": {
        "id": "qaZDrbSm6xAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: OpenAI"
      ],
      "metadata": {
        "id": "Jx-_ZKIeNePw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key= api_key,\n",
        ")\n",
        "\n",
        "# First API call with reasoning\n",
        "response = client.chat.completions.create(\n",
        "  model=\"openai/gpt-oss-20b:free\",\n",
        "  messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How many r's are in the word 'strawberry'?\"\n",
        "          }\n",
        "        ]\n",
        ")\n",
        "\n",
        "# Extract the assistant message with reasoning_details\n",
        "result = response.choices[0].message.content\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qxmfSJ9hOKvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTojJqr_OZZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}