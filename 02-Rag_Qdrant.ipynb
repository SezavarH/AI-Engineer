{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsWEbaqFdSIt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers qdrant-client torch accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple RAG\n",
        "\n",
        "\n",
        "Step 1: offline > Add data to vector DB (Qdrant)\n",
        "\n",
        "define model , embedding model, vectorDB\n",
        "\n",
        "create collection, prepare documents, insert emb(doc) into the collection\n",
        "\n",
        "create three functions for Retrieval, Prompt, Generation"
      ],
      "metadata": {
        "id": "7u4rh-vAw0fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ============================= offline ================================\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "\n",
        "qdrant = QdrantClient(\":memory:\")\n",
        "\n",
        "# Create a Collection\n",
        "qdrant.create_collection(\n",
        "    collection_name=\"documents\",\n",
        "    vectors_config=VectorParams(\n",
        "        size=384,\n",
        "        distance=Distance.COSINE\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"[TEST] collection created\")\n",
        "\n",
        "\n",
        "\n",
        "# add to collection: create vectors and then upsert into vectorDB\n",
        "documents = [\n",
        "    \"RAG stands for Retrieval Augmented Generation.\",\n",
        "    \"Qdrant is a vector database optimized for similarity search.\",\n",
        "    \"Transformers library provides open source language models.\",\n",
        "    \"Sentence Transformers generate embeddings for text.\"\n",
        "]\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "points = []\n",
        "for idx, doc in enumerate(documents):\n",
        "    embedding = embedder.encode(doc)\n",
        "\n",
        "    points.append(\n",
        "        PointStruct(\n",
        "            id=idx,\n",
        "            vector=embedding,\n",
        "            payload={\"text\": doc}\n",
        "        )\n",
        "    )\n",
        "\n",
        "qdrant.upsert(\n",
        "    collection_name=\"documents\",\n",
        "    points=points\n",
        ")\n",
        "\n",
        "## ============================= Core ================================\n",
        "def retrieve(query, top_k=3):          # encoder query, retrieve related vector and get payload\n",
        "    # 1. Encode query\n",
        "    query_vector = embedder.encode(query)\n",
        "\n",
        "    # 2. Search Qdrant\n",
        "    search_results = qdrant.query_points(\n",
        "        collection_name  = \"documents\",\n",
        "        query            = query_vector,\n",
        "        limit            = top_k\n",
        "    )\n",
        "\n",
        "    # 3. Extract text from payload\n",
        "    context = []\n",
        "    for point in search_results:\n",
        "            context.append(str(point))\n",
        "\n",
        "    return context\n",
        "\n",
        "def build_prompt(context, question):         # prompt is always a question, now it will contain the context also\n",
        "    context_text = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Use the following context to answer the question.\n",
        "\n",
        "    Context:\n",
        "    {context_text}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "def generate_answer(question):                                                  # Generation: call Retrieval, call Prompt, tokenize the prompt, pass tp the model and return answer\n",
        "    # 1. Retrieve text context\n",
        "    context = retrieve(question)\n",
        "\n",
        "    # 2. Build prompt\n",
        "    prompt = build_prompt(context, question)\n",
        "\n",
        "    # 3. Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # 4. Generate\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "    # 5. Decode\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "\n",
        "## ============================= Full RAG Test ================================\n",
        "\n",
        "question = \"What is RAG?\"\n",
        "print(f\"\\n{question}: {generate_answer(question)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Anc6OhxyRi",
        "outputId": "a1541118-2347-4eb0-dcac-e7570b259c75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] collection created\n",
            "\n",
            "What is RAG?: Retrieval Augmented Generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8tCmdyTfDJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}