{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2545b73",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc92d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-chroma \\\n",
    "  langchain-google-genai \\\n",
    "  chromadb \\\n",
    "  pypdf \\\n",
    "  python-dotenv \\\n",
    "  tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fa3e8",
   "metadata": {},
   "source": [
    "## Get Gemini API Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e9034d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20432d0",
   "metadata": {},
   "source": [
    "## Load PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cc592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 785 0 (offset 0)\n",
      "Ignoring wrong pointing object 1369 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 467 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "PDF_DIR = \"data/event_pdfs\"\n",
    "\n",
    "def load_pdfs(pdf_dir):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_dir):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            path = os.path.join(pdf_dir, file)\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "\n",
    "            # Attach paper ID to metadata\n",
    "            for d in docs:\n",
    "                d.metadata[\"paper_id\"] = file.replace(\".pdf\", \"\")\n",
    "            documents.extend(docs)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "raw_docs = load_pdfs(PDF_DIR)\n",
    "print(f\"Loaded {len(raw_docs)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06828675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/31 [00:02<00:07,  3.03it/s]Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 785 0 (offset 0)\n",
      "Ignoring wrong pointing object 1369 0 (offset 0)\n",
      "100%|██████████| 31/31 [00:30<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 467 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## load pdfs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = \"data/event_pdfs\"\n",
    "\n",
    "documents = []\n",
    "for file in tqdm(os.listdir(data_dir)):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        path = os.path.join(data_dir, file)\n",
    "\n",
    "        # loader: recieves path of a single file, returns list of Document objects  >> docs is a list\n",
    "        # Each Document object represents a page from the PDF\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Attach paper ID to metadata\n",
    "        for d in docs:\n",
    "            d.metadata[\"paper_id\"] = file.replace(\".pdf\", \"\")\n",
    "\n",
    "        # use extend to add multiple items to a list, not append to create nested list\n",
    "        documents.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac168ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Chunks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5ecfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num. chunks 6025\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================================================\n",
    "#                                               Documents into Chunk\n",
    "# ===============================================================================================================\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nnum. chunks {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d033968",
   "metadata": {},
   "source": [
    "### Explanation (don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What page_content might look like for page 1 of a research paper:\n",
    "docs[0].page_content = \"\"\"\n",
    "Event Cameras: A New Paradigm for Computer Vision\n",
    "John Doe, Jane Smith\n",
    "University of Technology\n",
    "\n",
    "Abstract—Event cameras are bio-inspired sensors...\n",
    "\n",
    "I. INTRODUCTION\n",
    "Traditional frame-based cameras...\n",
    "\n",
    "Figure 1: Comparison of event camera vs conventional camera.\n",
    "\"\"\"\n",
    "# ===========================================================================\n",
    "\n",
    "# Before your modification:\n",
    "docs[0].metadata = {\n",
    "    \"source\": \"data/event_pdfs/event_camera_survey.pdf\",  # File path\n",
    "    \"page\": 0  # Page number (0-indexed, so page 0 = actual page 1)\n",
    "}\n",
    "\n",
    "# After your modification with paper_id:\n",
    "docs[0].metadata = {\n",
    "    \"source\": \"data/event_pdfs/event_camera_survey.pdf\",\n",
    "    \"page\": 0,\n",
    "    \"paper_id\": \"event_camera_survey\"  # ← Your added field!\n",
    "}\n",
    "\n",
    "# ===========================================================================\n",
    "\n",
    "# documents list would contain 15 Document objects (5+3+7)\n",
    "len(documents)  # Returns: 15\n",
    "\n",
    "# Accessing specific documents:\n",
    "documents[0]   # Page 1 of event_camera_basics.pdf\n",
    "documents[4]   # Page 5 of event_camera_basics.pdf (last page of this PDF)\n",
    "documents[5]   # Page 1 of davis_sensors.pdf\n",
    "documents[7]   # Page 3 of davis_sensors.pdf (last page)\n",
    "documents[8]   # Page 1 of applications_robotics.pdf\n",
    "documents[14]  # Page 7 of applications_robotics.pdf (last page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efca010",
   "metadata": {},
   "source": [
    "## Create Embeddings + ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f05c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of embedding vectors 6025\n",
      "dimensions : 768\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "\n",
    "# Store in ChromaDB\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "DB_NAME = \"./vector_db_event_cameras\"  # Name of the ChromaDB directory\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "        Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents( documents=chunks, embedding=embeddings, persist_directory=DB_NAME )\n",
    "\n",
    "\n",
    "# print info about the stored vectors, _collection is the internal Chroma object\n",
    "collection = vectorstore._collection\n",
    "num_embedding = collection.count()\n",
    "print(f\"\\nnumber of embedding vectors {num_embedding}\")\n",
    "\n",
    "sample = collection.get(limit = 1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample)\n",
    "print(f\"dimensions : {dimensions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b59316",
   "metadata": {},
   "source": [
    "## Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a69dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Vector Store\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "DB_NAME =  \"./vector_db_event_cameras\"\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma(persist_directory = DB_NAME, embedding_function=embeddings)     # path, Embedding Model\n",
    "\n",
    "\n",
    "# when a query comes >> search and retrieve\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# define LLM model\n",
    "llm = ChatOllama(model=\"phi\",temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "SYSTEM_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a research assistant specializing in event-based vision.\n",
    "\n",
    "Use the following context from research papers to answer the question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (technical, concise, cite papers if possible):\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "151beda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": SYSTEM_PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b7d16c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " \n",
      "Event cameras are well suited for applications in which standard frame cameras are affected by motion blur, pixel saturation, and high latency. Despite the remarkable properties of event cameras, we are still at the dawn of event-based vision and their adoption in real systems is currently limited. This implies scarce availability of algorithms, datasets, and tools to manipulate and process events. Additionally, most of the available datasets have\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/event_pdfs\\NeurIPS-2020-learning-to-detect-objects-with-a-1-megapixel-event-camera-Paper.pdf\n",
      "data/event_pdfs\\Event-Based_Vision_A_Survey.pdf\n",
      "data/event_pdfs\\Event-Based_Vision_A_Survey.pdf\n",
      "data/event_pdfs\\Pan_Bringing_a_Blurry_Frame_Alive_at_High_Frame-Rate_With_an_CVPR_2019_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"How do event cameras differ from frame-based cameras in SLAM applications?\"\n",
    "\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.metadata.get(\"source\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c73303",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA\n",
    "# a pre-built chain in LangChain that combines: 1. Retrieval  2. Question Answering \n",
    "\n",
    "# chain_type=\"stuff\"\n",
    "# means: All documents concatenated into single prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
