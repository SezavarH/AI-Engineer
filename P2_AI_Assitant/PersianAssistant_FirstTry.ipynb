{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma - First try"
      ],
      "metadata": {
        "id": "tjTbK2PQfnzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "D_pYAbp2d9pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "23268806e86e4c879b8dad879472694d",
            "9523a2d5ee894b15956c768629ecffb9",
            "2de7b981d6d04289b9e2533a60979163",
            "ff5edaa1c6ab45a0b7804fdaae78f1ef",
            "5c0a4c37056b465fb8d07f81a311f367",
            "a155f1b2d5bb49dbb3544fe76ebc0f83",
            "dcadd0ed30e0439dabeff0a42b891494",
            "7fed09b8c7b24d4b8d786cedc417ce3e",
            "4722c5aa83f549f1a70a47887e9ee12b",
            "2163cc5284be4b18a41b601871bb0949",
            "88ed2c8459d7433f8af3941e5f22546a",
            "a080803390314c99a76f29458dd3dbe3",
            "a59e11211e28435eb9cdb4ed77c3e835",
            "e626ec56cf564c259f85f542e8f488fe",
            "4032d38bba324195883a09ef4b49db26",
            "4e749c2779cd4c548f487b69334c3538",
            "7e71a2af497942ada2ef36a816fe314d",
            "183c37ff2dde4cdaa36bfedd8e40a1a6",
            "f3e5e850c20b489385de7ec10672ff1f",
            "d2ef73e8ff2a4ed8ae3142d6cb45bbe6",
            "b1a27adf5a0f4cb8a5c4a76873ef7a10",
            "4f128b0b98994b6c99a2137e7668f97c",
            "ad4467bc0c7d42b7bb8cabd6c51ac46d",
            "bebca5a18c0c49ffb10c59e9cc39523c",
            "1f11fd3fe53749cfbbb5f5333b0c4ebb",
            "99a63069af7d45d09c37172561248be0",
            "e13b2b1dabf5485390d5ad78f7323e96",
            "c8dbf93d72704362876979513a94d077",
            "732b3e060d7a414995abeedc44e11177",
            "7d13dbb2c5874ddc8be75fad10eb0b4f",
            "b00debe3478a4ac2bdb9694c1f232ff7",
            "caa072d53b3c478d9448316f74e7f13b",
            "2143da0db2f84d1f8a1cd77d00df3911",
            "82065dabe0ad482f9013549d2f2b65cb",
            "c35e37efbfd24dd482e6f0128a4fa8df",
            "45cc3524f7ef40bc8d76cd90c3033dce",
            "36cb13590a374bc491ab08e25ec4dbb4",
            "df7c1bf836a64272aff8d4cb0771ba88",
            "df91cbf79400406d801426d615b982b2",
            "5ceb2040078942249eae7f69a7f60f38",
            "ebaf6863150e4dfa9c9464702be59538",
            "16b94accf03b4ecab0f48a451aa8670a",
            "4af04668c790437da1b2f40e7d387926",
            "2484dacf1dae4925a776985fc61a6a01",
            "c52aa78578ed4a769adaeb21a5520c53",
            "c2c034dc3be047efa6a45c4055d8a2c4",
            "c0e02b31dc70452cac0e2a06003edaee",
            "117dbc69f5aa4f45a5ba5cc5c752aece",
            "0cb65615253a473285d38922c138e0b6",
            "0d6a418ef0644c4288cd05fc03c640ee",
            "79a4a5e6ee094a12a68bf712154aa95d",
            "1d32ea31089746b9978677c5772b36a4",
            "ba2f35ce5bb748b2a5bb01bb555e6efd",
            "743a02434d1e424c941e3d472d70790a",
            "8dd2e85bdc6f4112b0d5216c17d7cfec"
          ]
        },
        "id": "ZIv197ucdY3w",
        "outputId": "8a48a702-0569-4203-904e-287d35428cf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23268806e86e4c879b8dad879472694d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a080803390314c99a76f29458dd3dbe3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad4467bc0c7d42b7bb8cabd6c51ac46d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82065dabe0ad482f9013549d2f2b65cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c52aa78578ed4a769adaeb21a5520c53"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get token from Colab Secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "model_id = \"google/gemma-2b-it\" # 'it' stands for instruction-tuned\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gemma(prompt, history=[]):\n",
        "    # Add the new user message to history\n",
        "    history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # NEW CODE (Fixed)\n",
        "    # 1. Generate the formatted tokens\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        history,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # 2. Pass only the input_ids to the generator\n",
        "    # Using **inputs unpacks both 'input_ids' and 'attention_mask'\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id # Added for stability\n",
        "    )\n",
        "\n",
        "    # Decode only the newly generated text\n",
        "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "    # Add model response to history for context\n",
        "    history.append({\"role\": \"model\", \"content\": response})\n",
        "    return response, history"
      ],
      "metadata": {
        "id": "yjBvLKtFdZ0M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "print(\"Gemma Bot: Hello! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit': break\n",
        "\n",
        "    response, chat_history = chat_with_gemma(user_input, chat_history)\n",
        "    print(f\"Gemma: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPP2Dpxte-5L",
        "outputId": "918462c5-fb6c-4c00-bfa5-1b97f66356d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemma Bot: Hello! Type 'exit' to stop.\n",
            "You: salam\n",
            "Gemma: Wa alaikumussalam wa rahmatullahi wa barakatuh. How may I assist you today?\n",
            "You: سلام\n",
            "Gemma: Wa salam wa rahmmatullahi wa barakatuh. How can I assist you today?\n",
            "You: فارسی می تونی حرف بزنی\n",
            "Gemma: No problem! I can help you with any questions or tasks you may have. What can I do for you?\n",
            "You: پایتون چیه\n",
            "Gemma: پایتون، من را بخواشت که دلیمش به service serve ملحوظی بتون معطوف.\n",
            "You: bye\n",
            "Gemma: Nailed it! I'm happy to assist you. How can I help?\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qwen - First Try"
      ],
      "metadata": {
        "id": "6lTXeAWLfsY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "537rlSNrfBXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# 4-bit configuration to keep it fast and fit in VRAM\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "0fbbc26a8e2743a5acbed7109abb0254",
            "2be90ce2d7fa4432b7ce95c960c28eba",
            "7408ae8191ca41c4a7ec67b105efa6c0",
            "f5d175f035c3420ca09b5e0835c55250",
            "8952e249fdb04e848cf6ed8aec690938",
            "91de8f7f080048cc8e342791770ebc6b",
            "cede5d4af17a4cd3924466f45576b7da",
            "eb5fecd29e454f5ba1b259711ec42eab",
            "7ec0629fab724f628d4ece3045298e5c",
            "7f44ceeea65a4bf7a4a3c645f9f5d5ca",
            "2f58bf6e26c1489a8f880ac9cd6660b5",
            "3013a788b8684a4f839f1cbc3bbc7d4f",
            "22b64a5325bc4ae18955a7ed15ff7a7a",
            "fe47e9c9e779460193bc2eb149c305c5",
            "b36dea5e3a63415ba107b915999547ac",
            "12bd549d2a7d4d89a112447c508b70b9",
            "5d765985d8e04ff7a3bc54c3b65e68a2",
            "7bdc6ac9e6674f9ca8e1c9ff4faaf1e1",
            "8e56f759df844aaa963f08d8866c9a67",
            "76df69b7bf104af89debb8a83fd6e917",
            "dd8039e4ba6141dbb9285143fed547f3",
            "9730fcf08c404461960af01f9751dfa6",
            "2d9bff198e714fd09461324faef7e40b",
            "3ab155b1309941beae161a4f4fc5da17",
            "52c4d6e9bd104e3599c80e44b721a337",
            "768bdf5ffb8347bfa0cc364bc2a90438",
            "ef7fede2e9d84005bb5567b658b71edb",
            "0d56808c5b4f4910bab8d39127578d76",
            "144c8fd196db4eaa9e64d829b13a26d1",
            "1316a776287f4f9b9ad8440e6d3941f8",
            "4415e9de661a475a87d353ecb4802b88",
            "14306c6f37a34cb6ab24c022700587c3",
            "f54fa1c056a34e12b79a87b64d2db1ea",
            "caa6cfceed25434298f0b4177d34bf1d",
            "ef2e1b63be0e4c068c7feb24944e65c2",
            "4051f0e90e1f40d1a0e7941ab68f1fdb",
            "71910ac78c144bd1a1005ef4068ad57b",
            "81ea0b59f40c4e0a8d561d7ab07cc085",
            "76509d035d52475f83cd8ea34b627184",
            "0ccf77e853ca4790b7359973ddd80a13",
            "23facc6315364e53a469e669b5516c56",
            "9ff121b6b5f04bd4bd0abe0c4f2d120a",
            "73a11dee4a3442ff82ef99ccd197f0df",
            "b19761e5104e4fb89f853b7d406cd104",
            "bd1967de73594486a474068ab34b3e71",
            "4e66b36fc70640c69a3ed7741025eca0",
            "0f805b9f97b54d13ab0df3d92feb66aa",
            "428dd53add4c406dbffef10e3dd5df66",
            "4b7d38ce51744fb1ac5aa8f114d100d7",
            "91e74a141add42c49da1ec98d5f13214",
            "eb8089a1db4c4f6f9da0542a4f63c602",
            "0db37f4bf72645d6b782f6be985cf9fe",
            "83bfa247605a4de6a721433e173960e7",
            "0577e3fbfc6d4aa78bebd83528be4173",
            "b9540d9f2b90446b9bc532a823853099",
            "5820a7d8d0d24041885250dd0333ab10",
            "9ce7ce28181d46ca8079206d5805bfce",
            "1adea81871d14612a80e471c8c4acd34",
            "daf59366e03448d6834d7fe94a02d884",
            "07ccdaffb7fa42f998dbf9e3cace4d7e",
            "1358772af69b4045bab2ecf1af94623e",
            "59a05c208b7b4bae8501597b0e0a1451",
            "ba981e931f244c33862ef454b2812516",
            "b33adc4614b742a18178fd7d345d6f99",
            "a34a1a5da28e4ff3a8520181d5c7edd1",
            "20c79013f03e40548da27805a1c37ee0",
            "3c3f10805c884315af282c84d68de338",
            "3b1007522fa94b3c9d9054ef0d454198",
            "59a7418bfa0044d0901d16373a090a17",
            "901e26d85cc34cfd8990540388332b39",
            "3eda0d58137b40248c0bb66cff515394",
            "6c7358317cdd4ddf80179bd93c5d2af2",
            "42a2b63a595f4efd90484181ec687441",
            "1a7b42e0fb5148d19a1e7c553a92a471",
            "45ff3fec9f09400580341d70ff47df79",
            "61077c63f9ee4678ac0e0316feab3d39",
            "8d3197a1ec0147c68c421ec9738d76f9",
            "397c5f3f5bc64fdb89f4bbe526807ae2",
            "33ae6f98bd524b41b105f503826fc23a",
            "066f0bc0ab2c4cc5aad58dcf3faebb8e",
            "159b918e87a049cbb537a02b567f9c8f",
            "384c68f357034cba89ba9129f7836820",
            "67276bd036704edaa5ac1cf61fb01623",
            "aca3c3e622cb42099ce9155d5c9d4b69",
            "dbcae4bee4164915a957387250013517",
            "88c8074208eb43f19b3cd17b031e91c6",
            "18300139e260420a92287bba19defb0d",
            "80b2284f911340f7b7e207102e164c73",
            "0c782691de4547e1a05fb4a6b6e924aa",
            "c6f8cd82c5164f2f8074deb8a02f4129",
            "912f8796cb614a14903c265e5077645c",
            "3dbea5e365d84911973fd7854e8c7c1a",
            "b7bd7350177d4bad9ad9795ca3f42f61",
            "f2593eec8a1848e1a451b549894bfe48",
            "55e45ce6dc1446e2a139ca4ba6cc7c3d",
            "4509f5bc251b40a985c747e2100bd64e",
            "110a2d74356146f9bd7daecf2215ac40",
            "f3dfe2fa9fd44f2581731dfa5b9c6b2c",
            "f9c97c622a904d33a76db8dd82b5c755",
            "00a579d805724327956d51c09839d5e2",
            "3324b7feefcd46918fa1ba8922881ba2",
            "a24f50baa0ba4cff9c28745594717ced",
            "dc38ce4d82b441588df8ab71e6005fbc",
            "04369176272043208e7f30c9b793e62b",
            "a1197766bc574661881bb2d894c25fff",
            "37b862d18a1b4b64916bc3b300d9c5a9",
            "e464bc7b64e74ad88b33f382ce59cfb2",
            "689412d0bdce41df80f023884a70297e",
            "d8eb70af02744cd9b174b29db4abc375",
            "980ced133f61458a935ea3118544f772"
          ]
        },
        "id": "vvFIW9rBf0R5",
        "outputId": "5b370086-c72d-48b6-ff10-c3f93e72a56e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fbbc26a8e2743a5acbed7109abb0254"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3013a788b8684a4f839f1cbc3bbc7d4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d9bff198e714fd09461324faef7e40b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caa6cfceed25434298f0b4177d34bf1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd1967de73594486a474068ab34b3e71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5820a7d8d0d24041885250dd0333ab10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3f10805c884315af282c84d68de338"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "397c5f3f5bc64fdb89f4bbe526807ae2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c782691de4547e1a05fb4a6b6e924aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a579d805724327956d51c09839d5e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_qwen(prompt, history=[]):\n",
        "    # System prompt ensures it stays in Persian mode\n",
        "    if not history:\n",
        "        history.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond in Persian (Farsi).\"})\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # NEW CODE (Fixed)\n",
        "    # 1. Generate the formatted tokens\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        history,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # 2. Pass the unpacked dictionary to the generator\n",
        "    # Using **inputs sends both 'input_ids' and 'attention_mask' correctly\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id # Added for stability\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "    return response, history"
      ],
      "metadata": {
        "id": "9xpTrtuzf27Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "print(\"Qwen Bot: سلام! چطور می‌توانم به شما کمک کنم؟ (برای خروج 'exit' بنویسید)\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"شما: \")\n",
        "    if user_input.lower() in ['exit', 'خروج']: break\n",
        "\n",
        "    response, chat_history = chat_with_qwen(user_input, chat_history)\n",
        "    print(f\"کوین: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIjpSN1TghWX",
        "outputId": "cceea0f6-b911-47e4-8de8-8ce9afcaa1af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen Bot: سلام! چطور می‌توانم به شما کمک کنم؟ (برای خروج 'exit' بنویسید)\n",
            "شما: سلام در یک خط بهم بگو پایتون چیه\n",
            "کوین: پایتون یک زبان برنامه‌نویسی عالی و آسان برای یادگیری است که برای توسعه نرم‌افزارها، تحلیل داده و اینچه‌ی دیگر استفاده می‌شود.\n",
            "شما: تابع چی هست\n",
            "کوین: تابع در پایتون (و دیگر زبان‌های برنامه‌نویسی) یک سیستم برای تغییرات جدید و تکراری انجام دادن کارها است. شما می‌توانید یک قسمت کد را که ممکن است در چند مکان مختلف نیاز به اجرای آن داشته باشید، به یک تابع بگذارید تا آن را یکبار نوشته و در مکان‌های مختلف خود را فراخوانی کنید.\n",
            "شما: پایتخت ایران کجاست؟\n",
            "کوین: پایتخت ایران تهران است.\n",
            "شما: فاصله ایران تا پرتغال چقدر هست؟\n",
            "کوین: فاصله جغرافیایی میان ایران و پرتغال به صورت غیر مستقیم است، زیرا این دو کشور در اروپا و آسیا قرار دارند. با این حال، با توجه به مسیر مسیریابی، می‌توان گفت که فاصله میان این دو کشور حدود 6,000 کیلومتر است. این فاصله معمولاً در طی راه‌های هوایی محاسبه می‌شود.\n",
            "شما: خطوط هواپیمایی معروف دنیا رو بهم بگو\n",
            "کوین: برخی خطوط هواپیمایی معروف دنیا عبارتند از:\n",
            "\n",
            "1. **آیرلاین‌های آمریکایی:**\n",
            "   - **آی‌جی‌ای (American Airlines):** یکی از بزرگترین خطوط هواپیمایی در ایالات متحده.\n",
            "   - **دلتا (Delta Air Lines):** دیگر یکی از بزرگترین خطوط هواپیمایی در آمریکا.\n",
            "   - **ناتیونال (United Airlines):** همچنین یکی از بزرگترین خطوط هواپیمایی در ایالات متحده.\n",
            "\n",
            "2. **آیرلاین‌های اروپایی:**\n",
            "   - **آی‌آی‌جی (Iberia):** خطوط هواپیمایی اسپانیا.\n",
            "   - **آرلاین‌ز (Ryanair):** یکی از بزرگترین خطوط هواپیمایی پیمانکاری در اروپا.\n",
            "   - **لیجاندرو (Lufthansa):** خطوط هواپیمایی آلمان.\n",
            "   - **آریل‌دی‌ئی‌جی (Air France):** یکی از بزرگترین خطوط هواپیمایی فرانسه.\n",
            "\n",
            "3. **آیرلاین‌های آسیایی:**\n",
            "   - **آسیا (AirAsia):** یکی از بزرگترین خطوط هواپیمایی در آسیا.\n",
            "   - **آسیا‌تی‌ای (Asiana Airlines):** خطوط هواپیمایی کره جنوبی.\n",
            "   - **آئی‌تی‌ای‌جی (Thai Airways International):** خطوط هواپیمایی تایلند.\n",
            "\n",
            "4. **آیرلاین‌های ایرانی:**\n",
            "   - **آی‌آی‌آر (Iran Aseman Airlines):** یک\n",
            "شما: خروج\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HoAoD9TNgy5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}